{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LLMFactory\n",
    "\n",
    "> We recommend use LLMFactory to create your LLM after v1.16.0. pne.ChatOpenAI, pne.ZhipuAI and pne.Qianfan... will be deprecated in the future. \n",
    "\n",
    "LLMFactory is a factory class to create LLM model. It integrates the ability of [litellm](https://github.com/BerriAI/litellm). It means you can call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs). Now let's take a look at how to use it.\n",
    "\n",
    "The following example show how to create an OpenAI model and chat."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8d41ea00991cca0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import promptulate as pne\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
    "model = pne.LLMFactory.build(model_name=\"gpt-3.5-turbo\", model_config={\n",
    "    \"temperature\": 0.5,\n",
    "})"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-29T15:26:47.768667Z",
     "start_time": "2024-04-29T15:26:39.550739Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "resp: str = model(\"hello, how are you?\")\n",
    "print(resp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T15:26:56.351688Z",
     "start_time": "2024-04-29T15:26:53.084169Z"
    }
   },
   "id": "c55f4cf323489372",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What's different between LLMFactory and pne.chat() ? \n",
    "\n",
    "At most time, you don't need to use LLMFactory directly. If you are developing a chatbot, you can use pne.chat() to chat and make a structure of response. Eg:\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "import promptulate as pne\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    provinces: List[str] = Field(description=\"All provinces in China\")\n",
    "\n",
    "\n",
    "response: LLMResponse = pne.chat(\n",
    "    messages=\"Please tell me all provinces in China.\",\n",
    "    output_schema=LLMResponse,\n",
    "    model=\"gpt-4-1106-preview\"\n",
    ")\n",
    "print(response.provinces)\n",
    "```\n",
    "\n",
    "pne.chat() covers 90% of development scenarios, so we recommend using pne.chat() if there are no special needs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2941b48fb56b7f18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to custom a LLM model?\n",
    "\n",
    "You can see the detail in [CustomLLM](https://undertone0809.github.io/promptulate/#/modules/llm/custom_llm?id=custom-llm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b83e9e0e4f4a273"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
